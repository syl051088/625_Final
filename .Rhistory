library(rpart.plot)
library(pROC)
library(doParallel)
# For reproducible results
set.seed(123)
# Adjust file paths as needed
train_data <- read.csv("~/Desktop/625_Final/data/train_data.csv")
test_data  <- read.csv("~/Desktop/625_Final/data/test_data.csv")
# Convert numeric Diabetes_binary to factor: 0 = No, 1 = Yes
train_data$Diabetes_binary <- factor(train_data$Diabetes_binary, levels = c("0", "1"), labels = c("No", "Yes"))
test_data$Diabetes_binary  <- factor(test_data$Diabetes_binary,  levels = c("0", "1"), labels = c("No", "Yes"))
# Check class distribution
table(train_data$Diabetes_binary)
table(test_data$Diabetes_binary)
# Detect cores and create cluster
num_cores <- parallel::detectCores() - 1
cl <- makeCluster(num_cores)
registerDoParallel(cl)
cat("Parallel backend registered with", num_cores, "cores.\n")
train_control <- trainControl(
method = "cv",
number = 5,              # Fewer folds = faster training (typical: 5 or 10)
classProbs = TRUE,       # Needed for ROC
summaryFunction = twoClassSummary,
allowParallel = TRUE
)
# If you want faster runs, reduce the range/step of cp
cp_grid <- expand.grid(cp = seq(0.0005, 0.02, by = 0.0005))
system.time({
dt_model <- train(
Diabetes_binary ~ .,
data      = train_data,
method    = "rpart",
metric    = "ROC",        # or "Accuracy"
trControl = train_control,
tuneGrid  = cp_grid
)
})
cat("Best CP found:", dt_model$bestTune$cp, "\n")
# Predictions and probabilities
dt_preds <- predict(dt_model, newdata = test_data)
dt_probs <- predict(dt_model, newdata = test_data, type = "prob")[, "Yes"]
# Confusion Matrix
dt_conf_mat <- confusionMatrix(dt_preds, test_data$Diabetes_binary, positive = "Yes")
print(dt_conf_mat)
# Metrics
accuracy    <- dt_conf_mat$overall["Accuracy"]
sensitivity <- dt_conf_mat$byClass["Sensitivity"]
specificity <- dt_conf_mat$byClass["Specificity"]
precision   <- dt_conf_mat$byClass["Pos Pred Value"]
f1_score    <- 2 * (precision * sensitivity) / (precision + sensitivity)
cat("Accuracy:",    round(accuracy, 4),    "\n")
cat("Sensitivity:", round(sensitivity, 4), "\n")
cat("Specificity:", round(specificity, 4), "\n")
cat("Precision:",   round(precision, 4),   "\n")
cat("F1 Score:",    round(f1_score, 4),    "\n")
# AUC
roc_obj <- roc(test_data$Diabetes_binary, dt_probs)
cat("AUC:", round(auc(roc_obj), 4), "\n")
# Plotting a large tree can be memory-intensive for big datasets.
rpart.plot(
dt_model$finalModel,
type = 2,
extra = 104,
fallen.leaves = TRUE,
main = "Decision Tree for Diabetes Prediction"
)
var_imp <- varImp(dt_model, scale = FALSE)
print(var_imp)
stopCluster(cl)
registerDoSEQ()
cat("Parallel backend stopped.\n")
# Load necessary libraries
library(dplyr)
library(caret)
library(rpart)
library(rpart.plot)
library(pROC)
library(doParallel)
# For reproducible results
set.seed(123)
# Adjust file paths as needed
train_data <- read.csv("~/Desktop/625_Final/data/train_data.csv")
test_data  <- read.csv("~/Desktop/625_Final/data/test_data.csv")
# Convert numeric Diabetes_binary to factor: 0 = No, 1 = Yes
train_data$Diabetes_binary <- factor(train_data$Diabetes_binary, levels = c("0", "1"), labels = c("No", "Yes"))
test_data$Diabetes_binary  <- factor(test_data$Diabetes_binary,  levels = c("0", "1"), labels = c("No", "Yes"))
# Check class distribution
table(train_data$Diabetes_binary)
table(test_data$Diabetes_binary)
# Detect cores and create cluster
num_cores <- parallel::detectCores() - 1
cl <- makeCluster(num_cores)
registerDoParallel(cl)
cat("Parallel backend registered with", num_cores, "cores.\n")
train_control <- trainControl(
method = "cv",
number = 5,              # Fewer folds = faster training (typical: 5 or 10)
classProbs = TRUE,       # Needed for ROC
summaryFunction = twoClassSummary,
allowParallel = TRUE
)
# If you want faster runs, reduce the range/step of cp
cp_grid <- expand.grid(cp = seq(0.0005, 0.02, by = 0.0005))
system.time({
dt_model <- train(
Diabetes_binary ~ .,
data      = train_data,
method    = "rpart",
metric    = "ROC",        # or "Accuracy"
trControl = train_control,
tuneGrid  = cp_grid
)
})
cat("Best CP found:", dt_model$bestTune$cp, "\n")
# Predictions and probabilities
dt_preds <- predict(dt_model, newdata = test_data)
dt_probs <- predict(dt_model, newdata = test_data, type = "prob")[, "Yes"]
# Confusion Matrix
dt_conf_mat <- confusionMatrix(dt_preds, test_data$Diabetes_binary, positive = "Yes")
print(dt_conf_mat)
# Metrics
accuracy    <- dt_conf_mat$overall["Accuracy"]
sensitivity <- dt_conf_mat$byClass["Sensitivity"]
specificity <- dt_conf_mat$byClass["Specificity"]
precision   <- dt_conf_mat$byClass["Pos Pred Value"]
f1_score    <- 2 * (precision * sensitivity) / (precision + sensitivity)
cat("Accuracy:",    round(accuracy, 4),    "\n")
cat("Sensitivity:", round(sensitivity, 4), "\n")
cat("Specificity:", round(specificity, 4), "\n")
cat("Precision:",   round(precision, 4),   "\n")
cat("F1 Score:",    round(f1_score, 4),    "\n")
# AUC
roc_obj <- roc(test_data$Diabetes_binary, dt_probs)
cat("AUC:", round(auc(roc_obj), 4), "\n")
# Plotting a large tree can be memory-intensive for big datasets.
rpart.plot(
dt_model$finalModel,
type = 2,
extra = 104,
fallen.leaves = TRUE,
main = "Decision Tree for Diabetes Prediction"
)
stopCluster(cl)
registerDoSEQ()
cat("Parallel backend stopped.\n")
# Load necessary libraries
library(dplyr)
library(caret)
library(rpart)
library(rpart.plot)
library(pROC)
library(doParallel)
# Reproducibility
set.seed(123)
# Load data (adjust paths as needed)
train_data <- read.csv("~/Desktop/625_Final/data/train_data.csv")
test_data  <- read.csv("~/Desktop/625_Final/data/test_data.csv")
# Convert numeric target to factor: 0 = No, 1 = Yes
train_data$Diabetes_binary <- factor(train_data$Diabetes_binary, levels = c("0", "1"), labels = c("No", "Yes"))
test_data$Diabetes_binary  <- factor(test_data$Diabetes_binary, levels = c("0", "1"), labels = c("No", "Yes"))
# Parallel setup
num_cores <- parallel::detectCores() - 1
cl <- makeCluster(num_cores)
registerDoParallel(cl)
# Define cross-validation and grid
train_control <- trainControl(
method = "cv",
number = 5,               # 5-fold CV for speed & reliability
classProbs = TRUE,
summaryFunction = twoClassSummary,
allowParallel = TRUE
)
# Tuning grid for complexity parameter
cp_grid <- expand.grid(cp = seq(0.0005, 0.02, by = 0.0005))
# Train Decision Tree with parallel CV
dt_model <- train(
Diabetes_binary ~ .,
data      = train_data,
method    = "rpart",
metric    = "ROC",       # or "Accuracy"
trControl = train_control,
tuneGrid  = cp_grid
)
# Predictions
dt_preds <- predict(dt_model, newdata = test_data)
dt_probs <- predict(dt_model, newdata = test_data, type = "prob")[, "Yes"]
# Confusion Matrix & Metrics
dt_conf_mat <- confusionMatrix(dt_preds, test_data$Diabetes_binary, positive = "Yes")
accuracy    <- dt_conf_mat$overall["Accuracy"]
sensitivity <- dt_conf_mat$byClass["Sensitivity"]
specificity <- dt_conf_mat$byClass["Specificity"]
precision   <- dt_conf_mat$byClass["Pos Pred Value"]
f1_score    <- 2 * (precision * sensitivity) / (precision + sensitivity)
cat("Best CP:", dt_model$bestTune$cp, "\n")
cat("Accuracy:", round(accuracy, 4), "\n")
cat("Sensitivity:", round(sensitivity, 4), "\n")
cat("Specificity:", round(specificity, 4), "\n")
cat("Precision:", round(precision, 4), "\n")
cat("F1 Score:", round(f1_score, 4), "\n")
# AUC
roc_obj <- roc(test_data$Diabetes_binary, dt_probs)
cat("AUC:", round(auc(roc_obj), 4), "\n")
# Optional: Tree Plot (can be memory-intensive)
# rpart.plot(dt_model$finalModel, type = 2, extra = 104, fallen.leaves = TRUE)
# Stop parallel cluster
stopCluster(cl)
registerDoSEQ()
# Load necessary libraries
library(dplyr)
library(caret)
library(rpart)
library(rpart.plot)
library(pROC)
library(doParallel)
# For reproducible results
set.seed(123)
# Adjust file paths as needed
train_data <- read.csv("~/Desktop/625_Final/data/train_data.csv")
test_data  <- read.csv("~/Desktop/625_Final/data/test_data.csv")
# Convert numeric Diabetes_binary to factor: 0 = No, 1 = Yes
train_data$Diabetes_binary <- factor(train_data$Diabetes_binary, levels = c("0", "1"), labels = c("No", "Yes"))
test_data$Diabetes_binary  <- factor(test_data$Diabetes_binary,  levels = c("0", "1"), labels = c("No", "Yes"))
# Check class distribution
table(train_data$Diabetes_binary)
table(test_data$Diabetes_binary)
num_cores <- parallel::detectCores() - 1
cl <- makeCluster(num_cores)
registerDoParallel(cl)
cat("Parallel backend registered with", num_cores, "cores.\n")
train_control <- trainControl(
method = "cv",
number = 5,
classProbs = TRUE,
summaryFunction = twoClassSummary,
allowParallel = TRUE
)
# Grid for complexity parameter tuning
cp_grid <- expand.grid(cp = seq(0.0005, 0.02, by = 0.0005))
dt_model <- train(
Diabetes_binary ~ .,
data      = train_data,
method    = "rpart",
metric    = "ROC",
trControl = train_control,
tuneGrid  = cp_grid
)
cat("Best CP found:", dt_model$bestTune$cp, "\n")
dt_preds <- predict(dt_model, newdata = test_data)
dt_probs <- predict(dt_model, newdata = test_data, type = "prob")[, "Yes"]
dt_conf_mat <- confusionMatrix(dt_preds, test_data$Diabetes_binary, positive = "Yes")
print(dt_conf_mat)
accuracy    <- dt_conf_mat$overall["Accuracy"]
sensitivity <- dt_conf_mat$byClass["Sensitivity"]
specificity <- dt_conf_mat$byClass["Specificity"]
precision   <- dt_conf_mat$byClass["Pos Pred Value"]
f1_score    <- 2 * (precision * sensitivity) / (precision + sensitivity)
cat("Accuracy:",    round(accuracy, 4), "\n")
cat("Sensitivity:", round(sensitivity, 4), "\n")
cat("Specificity:", round(specificity, 4), "\n")
cat("Precision:",   round(precision, 4), "\n")
cat("F1 Score:",    round(f1_score, 4), "\n")
# ROC and AUC
roc_obj <- roc(test_data$Diabetes_binary, dt_probs)
cat("AUC:", round(auc(roc_obj), 4), "\n")
rpart.plot(
dt_model$finalModel,
type = 2,
extra = 104,
fallen.leaves = TRUE,
main = "Decision Tree for Diabetes Prediction"
)
stopCluster(cl)
registerDoSEQ()
library(ggplot2)
model_accuracy <- data.frame(
Model    = c("Logistic Regression", "KNN", "Decision Tree", "Random Forest", "XGBoost"),
Accuracy = c(0.70, 0.72, 0.7376, 0.7481, 0.75),
Speedup  = c("3x Faster", "62% Faster", "Parallelized CV", "362s â†’ 3.42s", "Optimized Parallel Trees")
)
model_accuracy$Model <- factor(
model_accuracy$Model,
levels = model_accuracy$Model[order(model_accuracy$Accuracy, decreasing = TRUE)]
)
ggplot(model_accuracy, aes(x = Model, y = Accuracy, fill = Model)) +
geom_col(width = 0.6) +
geom_text(aes(label = paste(Accuracy, "\n", Speedup), y = Accuracy + 0.015), size = 3.5) +
labs(
title = "Accuracy and Speed Improvements Across Models (CDC Diabetes)",
x = NULL,
y = "Accuracy"
) +
theme_minimal(base_size = 12) +
theme(
axis.text.x = element_text(angle = 45, hjust = 1),
legend.position = "none",
plot.title = element_text(hjust = 0.5)
)
library(ggplot2)
model_accuracy <- data.frame(
Model    = c("Logistic Regression", "KNN", "Decision Tree", "Random Forest", "XGBoost"),
Accuracy = c(0.70, 0.72, 0.7376, 0.7481, 0.75)
)
model_accuracy$Model <- factor(
model_accuracy$Model,
levels = model_accuracy$Model[order(model_accuracy$Accuracy, decreasing = TRUE)]
)
ggplot(model_accuracy, aes(x = Model, y = Accuracy, fill = Model)) +
geom_col(width = 0.6) +
geom_text(aes(label = Accuracy, y = Accuracy + 0.015), size = 3.5) +
labs(
title = "Accuracy Comparison Across Models (CDC Diabetes)",
x = NULL,
y = "Accuracy"
) +
theme_minimal(base_size = 12) +
theme(
axis.text.x = element_text(angle = 45, hjust = 1),
legend.position = "none",
plot.title = element_text(hjust = 0.5)
) +
geom_label(
data = subset(model_accuracy, Accuracy == max(model_accuracy$Accuracy)),
aes(label = "Best Model", y = Accuracy + 0.05),
color = "white",
fill = "red",
fontface = "bold",
label.padding = unit(0.25, "lines"),
show.legend = FALSE
)
library(ggplot2)
model_accuracy <- data.frame(
Model    = c("Logistic Regression", "KNN", "Decision Tree", "Random Forest", "XGBoost"),
Accuracy = c(0.744, 0.72, 0.738, 0.748, 0.75)
)
model_accuracy$Model <- factor(
model_accuracy$Model,
levels = model_accuracy$Model[order(model_accuracy$Accuracy, decreasing = TRUE)]
)
ggplot(model_accuracy, aes(x = Model, y = Accuracy, fill = Model)) +
geom_col(width = 0.6) +
geom_text(aes(label = Accuracy, y = Accuracy + 0.015), size = 3.5) +
labs(
title = "Accuracy Comparison Across Models (CDC Diabetes)",
x = NULL,
y = "Accuracy"
) +
theme_minimal(base_size = 12) +
theme(
axis.text.x = element_text(angle = 45, hjust = 1),
legend.position = "none",
plot.title = element_text(hjust = 0.5)
) +
geom_label(
data = subset(model_accuracy, Accuracy == max(model_accuracy$Accuracy)),
aes(label = "Best Model", y = Accuracy + 0.05),
color = "white",
fill = "red",
fontface = "bold",
label.padding = unit(0.25, "lines"),
show.legend = FALSE
)
knitr::opts_chunk$set(echo=TRUE, warning=FALSE, message=FALSE)
library(dplyr)
library(caret)
library(rpart)
library(rpart.plot)
library(pROC)
library(doParallel)
set.seed(123)
train_data <- read.csv("~/Desktop/625_Final/data/train_data.csv")
test_data  <- read.csv("~/Desktop/625_Final/data/test_data.csv")
train_data$Diabetes_binary <- factor(train_data$Diabetes_binary, levels = c("0", "1"), labels = c("No", "Yes"))
test_data$Diabetes_binary  <- factor(test_data$Diabetes_binary,  levels = c("0", "1"), labels = c("No", "Yes"))
Non-Parallel Benchmark
train_data <- read.csv("~/Desktop/625_Final/data/train_data.csv")
test_data  <- read.csv("~/Desktop/625_Final/data/test_data.csv")
train_data$Diabetes_binary <- factor(train_data$Diabetes_binary, levels = c("0", "1"), labels = c("No", "Yes"))
test_data$Diabetes_binary  <- factor(test_data$Diabetes_binary,  levels = c("0", "1"), labels = c("No", "Yes"))
control_no_parallel <- trainControl(
method = "cv",
number = 5,
classProbs = TRUE,
summaryFunction = twoClassSummary,
allowParallel = FALSE  # No parallel
)
cp_grid <- expand.grid(cp = seq(0.0005, 0.02, by = 0.0005))
# Benchmark training time
time_non_parallel <- system.time({
dt_model_no_parallel <- train(
Diabetes_binary ~ .,
data      = train_data,
method    = "rpart",
metric    = "ROC",
trControl = control_no_parallel,
tuneGrid  = cp_grid
)
})
cat("Non-Parallel Decision Tree Training Time (seconds):", time_non_parallel["elapsed"], "\n")
num_cores <- parallel::detectCores() - 1
cl <- makeCluster(num_cores)
registerDoParallel(cl)
cat("Parallel backend registered with", num_cores, "cores.\n")
control_parallel <- trainControl(
method = "cv",
number = 5,
classProbs = TRUE,
summaryFunction = twoClassSummary,
allowParallel = TRUE  # Enable parallel
)
time_parallel <- system.time({
dt_model_parallel <- train(
Diabetes_binary ~ .,
data      = train_data,
method    = "rpart",
metric    = "ROC",
trControl = control_parallel,
tuneGrid  = cp_grid
)
})
cat("Parallel Decision Tree Training Time (seconds):", time_parallel["elapsed"], "\n")
stopCluster(cl)
registerDoSEQ()
speedup <- round(time_non_parallel["elapsed"] / time_parallel["elapsed"], 2)
cat("Speedup Factor:", speedup, "x faster with parallel.\n")
dt_preds <- predict(dt_model_parallel, newdata = test_data)
dt_probs <- predict(dt_model_parallel, newdata = test_data, type = "prob")[, "Yes"]
dt_conf_mat <- confusionMatrix(dt_preds, test_data$Diabetes_binary, positive = "Yes")
print(dt_conf_mat)
accuracy    <- dt_conf_mat$overall["Accuracy"]
sensitivity <- dt_conf_mat$byClass["Sensitivity"]
specificity <- dt_conf_mat$byClass["Specificity"]
precision   <- dt_conf_mat$byClass["Pos Pred Value"]
f1_score    <- 2 * (precision * sensitivity) / (precision + sensitivity)
cat("Accuracy:",    round(accuracy, 4), "\n")
cat("Sensitivity:", round(sensitivity, 4), "\n")
cat("Specificity:", round(specificity, 4), "\n")
cat("Precision:",   round(precision, 4), "\n")
cat("F1 Score:",    round(f1_score, 4), "\n")
roc_obj <- roc(test_data$Diabetes_binary, dt_probs)
cat("AUC:", round(auc(roc_obj), 4), "\n")
library(ggplot2)
model_accuracy <- data.frame(
Model    = c("Logistic Regression", "KNN", "Decision Tree", "Random Forest", "XGBoost"),
Accuracy = c(0.745, 0.72, 0.738, 0.748, 0.75)
)
model_accuracy$Model <- factor(
model_accuracy$Model,
levels = model_accuracy$Model[order(model_accuracy$Accuracy, decreasing = TRUE)]
)
ggplot(model_accuracy, aes(x = Model, y = Accuracy, fill = Model)) +
geom_col(width = 0.6) +
geom_text(aes(label = Accuracy, y = Accuracy + 0.015), size = 3.5) +
labs(
title = "Accuracy Comparison Across Models (CDC Diabetes)",
x = NULL,
y = "Accuracy"
) +
theme_minimal(base_size = 12) +
theme(
axis.text.x = element_text(angle = 45, hjust = 1),
legend.position = "none",
plot.title = element_text(hjust = 0.5)
) +
geom_label(
data = subset(model_accuracy, Accuracy == max(model_accuracy$Accuracy)),
aes(label = "Best Model", y = Accuracy + 0.05),
color = "white",
fill = "red",
fontface = "bold",
label.padding = unit(0.25, "lines"),
show.legend = FALSE
)
library(knitr)
contribution_data <- data.frame(
`Team Member` = c("Yulin Shao", "Liu Tong", "Yichen Zhao", "Yana Xu"),
`Contributions` = c("Logistic Regression, XGBoost", "K-Nearest Neighbors (KNN)", "Random Forest", "Decision Tree")
)
kable(contribution_data, caption = "Team Contributions", align = 'lc') %>%
kable_styling(latex_options = c("striped", "hold_position"))
library(knitr)
contribution_data <- data.frame(
`Team Member` = c("Yulin Shao", "Liu Tong", "Yichen Zhao", "Yana Xu"),
`Contributions` = c("Logistic Regression, XGBoost", "K-Nearest Neighbors (KNN)", "Random Forest", "Decision Tree")
)
kable(contribution_data, caption = "Team Contributions", align = 'lc') %>%
kable_styling(latex_options = c("striped", "hold_position"))
library(knitr)
library(kableExtra)
installed.packages("kableExtra")
install.packages("kableExtra")
library(knitr)
library(kableExtra)
library(dplyr)
contribution_data <- data.frame(
`Team Member` = c("Yulin Shao", "Liu Tong", "Yichen Zhao", "Yana Xu"),
`Contributions` = c("Logistic Regression, XGBoost", "K-Nearest Neighbors (KNN)", "Random Forest", "Decision Tree")
)
contribution_data %>%
kable(caption = "Team Contributions", align = 'lc') %>%
kable_styling(latex_options = c("striped", "hold_position"))
library(knitr)
library(kableExtra)
library(dplyr)
contribution_data <- data.frame(
`Team Member` = c("Yulin Shao", "Liu Tong", "Yichen Zhao", "Yana Xu"),
`Contributions` = c("Logistic Regression, XGBoost", "K-Nearest Neighbors (KNN)", "Random Forest", "Decision Tree")
)
contribution_data %>%
kable(caption = "Team Contributions", align = 'lc') %>%
kable_styling(latex_options = c("striped", "hold_position"))
View(train_data)
