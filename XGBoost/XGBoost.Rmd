---
title: "XGBoost Model"
author: "Yulin Shao"
date: "2024-10-30"
output: pdf_document
---

```{r setup, include=FALSE}
# Load necessary libraries
library(dplyr)
library(tidyverse)
library(pROC)
library(caret)
library(ggplot2)
library(doParallel)
library(bench)
library(xgboost)
library(Matrix)
```

```{r data processing}
# Load the dataset
train_data = read.csv("../data/train_data.csv")
dense_train <- as.matrix(train_data[, -which(names(train_data) == "Diabetes_binary")])
train_labels <- train_data$Diabetes_binary

test_data = read.csv("../data/test_data.csv")
dense_test <- as.matrix(test_data[, -which(names(test_data) == "Diabetes_binary")])
test_labels <- test_data$Diabetes_binary

# Convert the training and test data into sparse matrices
sparse_train <- as(dense_train, "dgCMatrix")
sparse_test <- as(dense_test, "dgCMatrix")

dense_dtrain <- xgb.DMatrix(data = dense_train, label = train_labels)
dense_dtest <- xgb.DMatrix(data = dense_test, label = test_labels)

sparse_dtrain <- xgb.DMatrix(data = sparse_train, label = train_labels)
sparse_dtest <- xgb.DMatrix(data = sparse_test, label = test_labels)
```


```{r xggL function}
xggTree <- function(dtrain, dtest, parallel = FALSE, eta_grid = c(0.01, 0.1, 0.3)) {
  # Set up parallelization
  n_threads <- if (parallel) parallel::detectCores() - 1 else 1
  
  # Initialize variables to store best parameters and performance
  best_eta <- 0
  best_auc <- 0
  best_nrounds <- 0
  
  # Fixed parameters for the tree-based booster
  fixed_params <- list(
    booster = "gbtree",             # Use tree-based booster
    objective = "binary:logistic",  # Binary classification
    eval_metric = "auc",            # AUC as evaluation metric
    max_depth = 6,                  # Tree depth (fixed)
    subsample = 0.8,                # Subsample ratio
    colsample_bytree = 0.8,         # Column sampling ratio
    nthread = n_threads             # Number of threads
  )
  
  # Hyperparameter tuning for eta
  for (eta_value in eta_grid) {
    params <- c(fixed_params, list(eta = eta_value))  # Update with current eta value
    
    # Perform cross-validation
    cv_results <- xgb.cv(
      params = params,
      data = dtrain,
      nrounds = 100,
      nfold = 5,
      early_stopping_rounds = 10,
      verbose = FALSE
    )
    
    # Extract the best AUC and number of rounds
    mean_auc <- max(cv_results$evaluation_log$test_auc_mean)
    nrounds <- cv_results$best_iteration
    
    # Update best parameters if current AUC is better
    if (mean_auc > best_auc) {
      best_auc <- mean_auc
      best_eta <- eta_value
      best_nrounds <- nrounds
    }
  }
  
  # Train the final model with the best eta
  final_model <- xgb.train(
    params = c(fixed_params, list(eta = best_eta)),
    data = dtrain,
    nrounds = best_nrounds,
    verbose = FALSE
  )
  
  # Make predictions on the test dataset
  pred_probs <- predict(final_model, newdata = dtest)
  
  # Define a threshold to classify predictions
  threshold <- 0.5
  binary_preds <- ifelse(pred_probs > threshold, 1, 0)
  
  # Calculate accuracy
  true_labels <- getinfo(dtest, "label")  # Extract true labels from the test data
  accuracy <- sum(binary_preds == true_labels) / length(true_labels)
  
  # Return accuracy as part of the results
  list(
    best_eta = best_eta,
    best_auc = best_auc,
    best_nrounds = best_nrounds,
    predictions = pred_probs,
    accuracy = accuracy,
    model = final_model
  )
}


```

```{r benchmark results}
library(microbenchmark)

# Initialize a list to store model results
model_results <- list()

# Benchmark and collect results
benchmark_results <- microbenchmark(
  dense_no_parallel = { model_results$dense_no_parallel <- xggTree(dense_dtrain, dense_dtest, parallel = FALSE) },
  dense_parallel = { model_results$dense_parallel <- xggTree(dense_dtrain, dense_dtest, parallel = TRUE) },
  sparse_no_parallel = { model_results$sparse_no_parallel <- xggTree(sparse_dtrain, sparse_dtest, parallel = FALSE) },
  sparse_parallel = { model_results$sparse_parallel <- xggTree(sparse_dtrain, sparse_dtest, parallel = TRUE) },
  times = 1
)

# Print time results
print(benchmark_results)
```

```{r ploting}
# Extract timing results from microbenchmark
time_data <- summary(benchmark_results)[, c("expr", "mean")]
colnames(time_data) <- c("Model", "Time_s")
time_data$Time_s <- time_data$Time_s

# Extract AUC and accuracy results from model outputs
auc_data <- data.frame(
  Model = names(model_results),
  AUC = sapply(model_results, function(res) res$best_auc),
  Accuracy = sapply(model_results, function(res) res$accuracy)  # Add accuracy
)

# Merge time, AUC, and Accuracy results for comparison
results_data <- merge(time_data, auc_data, by = "Model")
print(results_data)

# Plot Training Time
ggplot(results_data, aes(x = Model, y = Time_s, fill = Model)) +
  geom_bar(stat = "identity") +
  theme_minimal() +
  labs(
    title = "Training Time by Model",
    x = "Model",
    y = "Time (ms)"
  ) +
  theme(axis.text.x = element_text(angle = 45, hjust = 1)) +
  scale_fill_brewer(palette = "Set2")

# Plot AUC
ggplot(results_data, aes(x = Model, y = AUC, fill = Model)) +
  geom_bar(stat = "identity") +
  theme_minimal() +
  labs(
    title = "AUC by Model",
    x = "Model",
    y = "AUC"
  ) +
  theme(axis.text.x = element_text(angle = 45, hjust = 1)) +
  scale_fill_brewer(palette = "Set3")

# Plot Accuracy
ggplot(results_data, aes(x = Model, y = Accuracy, fill = Model)) +
  geom_bar(stat = "identity") +
  theme_minimal() +
  labs(
    title = "Accuracy by Model",
    x = "Model",
    y = "Accuracy"
  ) +
  theme(axis.text.x = element_text(angle = 45, hjust = 1)) +
  scale_fill_brewer(palette = "Pastel1")

# Generate ROC curves for all models
roc_list <- lapply(model_results, function(res) {
  roc(test_labels, res$predictions)
})

# Combine ROC data into a single data frame
roc_data <- data.frame()
for (model_name in names(roc_list)) {
  roc_curve <- roc_list[[model_name]]
  roc_points <- data.frame(
    FPR = 1 - roc_curve$specificities,
    TPR = roc_curve$sensitivities,
    Model = model_name
  )
  roc_data <- rbind(roc_data, roc_points)
}

# Plot ROC Curve
ggplot(roc_data, aes(x = FPR, y = TPR, color = Model)) +
  geom_line(size = 1) +
  theme_minimal() +
  labs(
    title = "ROC Curve Comparison",
    x = "False Positive Rate",
    y = "True Positive Rate"
  ) +
  scale_color_brewer(palette = "Dark2")

```


