---
title: "Logistic Regression"
author: "Yulin Shao"
date: "2024-10-30"
output: pdf_document
---

```{r setup, include=FALSE}
# Load necessary libraries
library(dplyr)
library(tidyverse)
library(pROC)
library(caret)
library(ggplot2)
library(doParallel)
library(bench)
library(xgboost)
library(Matrix)
```

```{r data processing}
# Load the dataset
train_data = read.csv("../data/train_data.csv")
dense_train <- as.matrix(train_data[, -which(names(train_data) == "Diabetes_binary")])
train_labels <- train_data$Diabetes_binary

test_data = read.csv("../data/test_data.csv")
dense_test <- as.matrix(test_data[, -which(names(test_data) == "Diabetes_binary")])
test_labels <- test_data$Diabetes_binary

# Convert the training and test data into sparse matrices
sparse_train <- as(dense_train, "dgCMatrix")
sparse_test <- as(dense_test, "dgCMatrix")

dense_dtrain <- xgb.DMatrix(data = dense_train, label = train_labels)
dense_dtest <- xgb.DMatrix(data = dense_test, label = test_labels)

sparse_dtrain <- xgb.DMatrix(data = sparse_train, label = train_labels)
sparse_dtest <- xgb.DMatrix(data = sparse_test, label = test_labels)
```


```{r xggL function}
xggTree <- function(dtrain, dtest, parallel = FALSE, eta_grid = c(0.01, 0.1, 0.3)) {
  # Set up parallelization
  n_threads <- if (parallel) parallel::detectCores() - 1 else 1
  
  # Initialize variables to store best parameters and performance
  best_eta <- 0
  best_auc <- 0
  best_nrounds <- 0
  
  # Fixed parameters for the tree-based booster
  fixed_params <- list(
    booster = "gbtree",             # Use tree-based booster
    objective = "binary:logistic",  # Binary classification
    eval_metric = "auc",            # AUC as evaluation metric
    max_depth = 6,                  # Tree depth (fixed)
    subsample = 0.8,                # Subsample ratio
    colsample_bytree = 0.8,         # Column sampling ratio
    nthread = n_threads             # Number of threads
  )
  
  # Hyperparameter tuning for eta
  for (eta_value in eta_grid) {
    params <- c(fixed_params, list(eta = eta_value))  # Update with current eta value
    
    # Perform cross-validation
    cv_results <- xgb.cv(
      params = params,
      data = dtrain,
      nrounds = 100,
      nfold = 5,
      early_stopping_rounds = 10,
      verbose = FALSE
    )
    
    # Extract the best AUC and number of rounds
    mean_auc <- max(cv_results$evaluation_log$test_auc_mean)
    nrounds <- cv_results$best_iteration
    
    # Update best parameters if current AUC is better
    if (mean_auc > best_auc) {
      best_auc <- mean_auc
      best_eta <- eta_value
      best_nrounds <- nrounds
    }
  }
  
  # Train the final model with the best eta
  final_model <- xgb.train(
    params = c(fixed_params, list(eta = best_eta)),
    data = dtrain,
    nrounds = best_nrounds,
    verbose = FALSE
  )
  
  # Make predictions on the test dataset
  pred_probs <- predict(final_model, newdata = dtest)
  
  # Return results
  list(
    best_eta = best_eta,
    best_auc = best_auc,
    best_nrounds = best_nrounds,
    predictions = pred_probs,
    model = final_model
  )
}


```

```{r}
library(microbenchmark)

results <- microbenchmark(
  dense_no_parallel = xggTree(dense_dtrain, dense_dtest, parallel = FALSE),
  dense_parallel = xggTree(dense_dtrain, dense_dtest, parallel = TRUE),
  sparse_no_parallel = xggTree(sparse_dtrain, sparse_dtest, parallel = FALSE),
  sparse_parallel = xggTree(sparse_dtrain, sparse_dtest, parallel = TRUE),
   times = 3
)

print(results)
```



```{r}
# Create DMatrix objects
dtrain <- xgb.DMatrix(data = train_matrix, label = train_labels)
dtest <- xgb.DMatrix(data = test_matrix, label = test_labels)

# Hyperparameter grid for learning rate tuning
eta_grid <- c(0.01, 0.05, 0.1, 0.2, 0.3)  # Define learning rate values

# Initialize variables to store best parameters and best AUC
best_params <- list()
best_auc <- 0
best_nrounds <- 0

# Grid search for learning rate tuning
for (eta_value in eta_grid) {
  params <- list(
    booster = "gbtree",              # Using tree-based booster
    objective = "binary:logistic",   # Binary classification
    eval_metric = "auc",             # Use AUC for evaluation
    eta = eta_value,                 # Varying learning rate
    lambda = 1,                      # Fixed L2 regularization
    alpha = 0                        # Fixed L1 regularization
  )
  
  # Perform cross-validation
  cv_results <- xgb.cv(
    params = params,
    data = dtrain,
    nrounds = 100,                   # Maximum number of boosting rounds
    nfold = 5,                       # 5-fold cross-validation
    early_stopping_rounds = 10,      # Stop if no improvement
    verbose = FALSE                  # Suppress output
  )
  
  # Extract the best AUC and number of rounds
  mean_auc <- max(cv_results$evaluation_log$test_auc_mean)
  nrounds <- cv_results$best_iteration
  
  # Update best parameters if current AUC is better
  if (mean_auc > best_auc) {
    best_auc <- mean_auc
    best_params <- params
    best_nrounds <- nrounds
  }
}

# Print best learning rate, AUC, and number of rounds
cat("Best Learning Rate (eta):", best_params$eta, "\n")
cat("Best AUC:", best_auc, "\n")
cat("Best Number of Rounds:", best_nrounds, "\n")

# Train the final model with the best learning rate
final_model <- xgb.train(
  params = best_params,
  data = dtrain,
  nrounds = best_nrounds,
  verbose = TRUE
)

# Make predictions on the test dataset
pred_probs <- predict(final_model, newdata = dtest)
pred_classes <- ifelse(pred_probs > 0.5, 1, 0)

# Evaluate the model on the test set
# Confusion Matrix
confusion <- confusionMatrix(
  factor(pred_classes, levels = c(0, 1)), 
  factor(test_labels, levels = c(0, 1))
)

# Print evaluation metrics
print(confusion)

# ROC Curve and AUC
roc_curve <- roc(test_labels, pred_probs)
auc_value <- auc(roc_curve)

# Plot ROC Curve
plot(roc_curve, col = "blue", main = "ROC Curve for XGBoost Model")
cat("AUC:", auc_value, "\n")
```

